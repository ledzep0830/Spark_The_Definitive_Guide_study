{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장에서는 단일 데이터셋의 집계 방법을 알아보았음<br/>\n",
    "하지만 스파크 애플리케이션에서는 다양한 데이터셋을 함께 결합해 사용하는 경우가 더 많음<br/>\n",
    "따라서 조인은 거의 모든 스파크 작업에 필수적으로 사용됨<br/>\n",
    "스파크는 서로 다른 데이터를 조합할 수 있으므로 데이터를 처리할 때 기업의 여러 데이터소스를 활용할 수 있음<br/>\n",
    "\n",
    "이 장에서는 스파크가 지원하는 조인 타입과 사용법 그리고 실제 스파크가 클러스터에서 어떻게 조인을 실행하는지 생각해볼 수 있도록 기본적인 내부 동작 방식을 다룸<br/>\n",
    "이러한 기초 지식은 *메모리 부족 상황을 회피하는 방법*과 이전에 풀지 못했던 문제를 해결하는 데 도움이 됨<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 조인 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 **왼쪽**과 **오른쪽** 데이터셋에 있는 하나 이상의 **키값**을 비교하고 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 조인 표현식(join expression)의 평가 결과에 따라 두 개의 데이터셋을 조인함<br/>\n",
    "가장 많이 사용하는 조인 표현식은 왼쪽과 오른쪽 데이터셋에 지정된 키가 동일한지 비교하는 동등 조인(equal-join)임<br/>\n",
    "키가 일치하면 스파크는 왼쪽과 오른쪽의 데이터셋을 결합함<br/>\n",
    "일치하지 않으면 데이터셋을 결합하지 않음<br/>\n",
    "스파크는 일치하는 키가 없는 로우는 조인에 포함시키지 않음<br/>\n",
    "스파크는 동등 조인뿐만 아니라 더 복잡한 조인 정책도 지원함<br/>\n",
    "또한 복합 데이터 타입을 조인에 사용할 수도 있음<br/>\n",
    "예를 들어 배열 타입의 키에 조인할 키가 존재하는지 확인해 조인을 수행할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 조인 타입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인 표현식은 두 로우의 조인 여부를 결정하는 반면 조인 타입은 결과 데이터셋에 어떤 데이터가 있어야 하는지 결정함<br/>\n",
    "스파크에서 사용할 수 있는 조인 타입은 다음과 같음<br/>\n",
    "* 내부 조인(inner join): 왼쪽과 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "* 외부 조인(outer join): 왼쪽이나 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "* 왼쪽 외부 조인(left outer join): 왼쪽 데이터셋에 키가 있는 로우를 유지\n",
    "* 오른쪽 외부 조인(right outer join): 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "* 왼쪽 세미 조인(left semi join): 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 키가 일치하는 왼쪽 데이터셋만 유지\n",
    "* 왼쪽 안티 조인(left anti join): 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터셋만 유지\n",
    "* 자연 조인(natural join): 두 데이터셋에서 동일한 이름을 가진 컬럼을 암시적(implicit)으로 결합하는 조인을 수행\n",
    "* 교차 조인(cross join) 또는 카테시안 조인(Cartesian join): 왼쪽 데이터셋의 모든 로우와 오른쪽 데이터셋의 모든 로우를 조합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 관계형 데이터베이스 시스템이나 엑셀 스프레드시트를 사용해봤다면 두 개의 데이터셋을 조인하는 것이 그리 어렵지 않을 것임<br/>\n",
    "이제 각 조인 타입에 대한 예제를 살펴보겠음<br/>\n",
    "그러면 문제를 해결하기 위해 정확하게 어떤 조인 타입을 사용해야 하는지 쉽게 이해할 수 있게 될 것임<br/>\n",
    "우선 예제에서 사용할 몇 가지 간단한 데이터셋을 만듦<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.7:4045\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1641459237248)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "person: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n",
       "graduateProgram: org.apache.spark.sql.DataFrame = [id: int, degree: string ... 2 more fields]\n",
       "sparkStatus: org.apache.spark.sql.DataFrame = [id: int, status: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person = Seq(\n",
    "    (0, \"Bill Chambers\", 0, Seq(100)),\n",
    "    (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n",
    "    (2, \"Michael Armbrust\", 1, Seq(250, 100)))\n",
    "    .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "val graduateProgram = Seq(\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D\", \"EECS\", \"UC Berkeley\"))\n",
    "    .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "val sparkStatus = Seq(\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\"))\n",
    "    .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 데이터셋을 이 장 전체 예제에서 사용하기 위해 테이블로 등록함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "|id |name            |graduate_program|spark_status   |\n",
      "+---+----------------+----------------+---------------+\n",
      "|0  |Bill Chambers   |0               |[100]          |\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------+-----------+\n",
      "|id |degree |department           |school     |\n",
      "+---+-------+---------------------+-----------+\n",
      "|0  |Masters|School of Information|UC Berkeley|\n",
      "|2  |Masters|EECS                 |UC Berkeley|\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM graduateProgram\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|id |status        |\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|PMC Member    |\n",
      "|100|Contributor   |\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sparkStatus\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 내부 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부 조인은 DataFrame이나 테이블에 존재하는 키를 평가함<br/>\n",
    "그리고 true로 평가되는 로우만 결합함<br/>\n",
    "다음은 *graduateProgram* DataFrame과 *person* DataFrame을 조인해 새로운 DataFrame을 만드는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpression: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 DataFrame 모두에 키가 존재하지 않으면 결과 DataFrame에서 볼 수 없음<br/>\n",
    "예를 들어 다음과 같은 표현식을 사용하면 비어 있는 결과 DataFrame을 얻게 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrongJoinExpression: org.apache.spark.sql.Column = (name = school)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부 조인은 기본 조인 방식이므로 JOIN 표현식에 왼쪽 DataFrame과 오른쪽 DataFrame을 지정하기만 하면 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |id |degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |0  |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join 메서드의 세 번째 파라미터(joinType)로 조인 타입을 명확하게 지정할 수도 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |id |degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |0  |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = inner\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var joinType = \"inner\"\n",
    "\n",
    "person.join(graduateProgram, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 외부 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가하여 true나 false로 평가한 로우를 포함(그리고 조인)함<br/>\n",
    "왼쪽이나 오른쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|id  |name            |graduate_program|spark_status   |id |degree |department           |school     |\n",
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|1   |Matei Zaharia   |1               |[500, 250, 100]|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|2   |Michael Armbrust|1               |[250, 100]     |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|null|null            |null            |null           |2  |Masters|EECS                 |UC Berkeley|\n",
      "|0   |Bill Chambers   |0               |[100]          |0  |Masters|School of Information|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = outer\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "\n",
    "person.join(graduateProgram, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 왼쪽 외부 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽 외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가함<br/>\n",
    "그리고 왼쪽 DataFrame의 모든 로우와 왼쪽 DataFrame과 일치하는 오른쪽 DataFrame의 로우를 함께 포함함<br/>\n",
    "오른쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------+-----------+----+----------------+----------------+---------------+\n",
      "|id |degree |department           |school     |id  |name            |graduate_program|spark_status   |\n",
      "+---+-------+---------------------+-----------+----+----------------+----------------+---------------+\n",
      "|0  |Masters|School of Information|UC Berkeley|0   |Bill Chambers   |0               |[100]          |\n",
      "|2  |Masters|EECS                 |UC Berkeley|null|null            |null            |null           |\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|2   |Michael Armbrust|1               |[250, 100]     |\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|1   |Matei Zaharia   |1               |[500, 250, 100]|\n",
      "+---+-------+---------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_outer\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "\n",
    "graduateProgram.join(person, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.6 오른쪽 외부 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오른쪽 외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가함<br/>\n",
    "그리고 오른쪽 DataFrame의 모든 로우와 오른쪽 DataFrame과 일치하는 왼쪽 DataFrame의 로우를 함께 포함함<br/>\n",
    "왼쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|id  |name            |graduate_program|spark_status   |id |degree |department           |school     |\n",
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|0   |Bill Chambers   |0               |[100]          |0  |Masters|School of Information|UC Berkeley|\n",
      "|null|null            |null            |null           |2  |Masters|EECS                 |UC Berkeley|\n",
      "|2   |Michael Armbrust|1               |[250, 100]     |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1   |Matei Zaharia   |1               |[500, 250, 100]|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = right_outer\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "\n",
    "person.join(graduateProgram, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.7 왼쪽 세미 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세미 조인은 오른쪽 DataFrame의 어떤 값도 포함하지 않기 때문에 다른 조인 탕비과는 약간 다름<br/>\n",
    "단지 두 번째 DataFrame은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용함<br/>\n",
    "만약 값이 존재한다면 왼쪽 DataFrame에 중복 키가 존재하더라도 해당 로우는 결과에 포함됨<br/>\n",
    "왼쪽 세미 조인은 기존 조인 기능과는 달리 DataFrame의 필터 정도로 볼 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------+-----------+\n",
      "|id |degree |department           |school     |\n",
      "+---+-------+---------------------+-----------+\n",
      "|0  |Masters|School of Information|UC Berkeley|\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_semi\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "\n",
    "graduateProgram.join(person, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgram2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, degree: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgram2 = graduateProgram.union(Seq(\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")).toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  1|   Ph.D|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.8 왼쪽 안티 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽 안티 조인은 왼쪽 세미 조인의 반대 개념임<br/>\n",
    "왼쪽 세미 조인처럼 오른쪽 DataFrame의 어떤 값도 포함하지 않음<br/>\n",
    "단지 두 번째 DataFrame은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용함<br/>\n",
    "하지만 두 번째 DataFrame에 존재하는 값을 유지하는 대신 두 번째 DataFrame에서 관련된 키를 찾을 수 없는 로우만 결과에 포함함<br/>\n",
    "안티 조인은 SQL의 NOT IN과 같은 스타일의 필터로 볼 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "|id |degree |department|school     |\n",
      "+---+-------+----------+-----------+\n",
      "|2  |Masters|EECS      |UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_anti\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "\n",
    "graduateProgram.join(person, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.9 자연 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연 조인은 조인하려는 컬럼을 암시적으로 추정함<br/>\n",
    "즉, 일치하는 컬럼을 찾고 그 결과를 반환함<br/>\n",
    "왼쪽과 오른쪽 그리고 외부 자연 조인을 사용할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CAUTION*<br/>\n",
    "암시적인 처리는 언제나 위험함<br/>\n",
    "아래 쿼리는 두 DataFrame 또는 테이블이 id라는 동일한 컬럼명을 가지지만 각각의 데이터셋 입장에서는 서로 다른 의미를 지니므로 부정확한 결과를 낳을 수 있음<br/>\n",
    "그렇기 때문에 자연 조인은 언제나 조심해서 사용해야 함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "-- SQL\n",
    "\n",
    "SELECT * FROM graduateProgram NATURAL JOIN person\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.10 교차 조인(카테시안 조인)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 알아볼 조인 타입은 교차 조인(또는 카테시안 조인)임<br/>\n",
    "간단히 말해, 교차 조인은 조건절을 기술하지 않은 내부 조인을 의미함<br/>\n",
    "교차 조인은 왼쪽 DataFrame의 모든 로우를 오른쪽 DataFrame의 모든 로우와 결합함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------------+-----------+---+----------------+----------------+---------------+\n",
      "|id |degree |department           |school     |id |name            |graduate_program|spark_status   |\n",
      "+---+-------+---------------------+-----------+---+----------------+----------------+---------------+\n",
      "|0  |Masters|School of Information|UC Berkeley|0  |Bill Chambers   |0               |[100]          |\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|2  |Michael Armbrust|1               |[250, 100]     |\n",
      "|1  |Ph.D   |EECS                 |UC Berkeley|1  |Matei Zaharia   |1               |[500, 250, 100]|\n",
      "+---+-------+---------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = cross\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "\n",
    "graduateProgram.join(person, joinExpression, joinType).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교차 조인이 필요한 경우 다음과 같이 명시적으로 메서드를 호출할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |id |degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |0  |Masters|School of Information|UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|0  |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |0  |Masters|School of Information|UC Berkeley|\n",
      "|0  |Bill Chambers   |0               |[100]          |2  |Masters|EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|2  |Masters|EECS                 |UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |2  |Masters|EECS                 |UC Berkeley|\n",
      "|0  |Bill Chambers   |0               |[100]          |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |1  |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.11 조인 사용 시 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인을 하다 보면 몇 가지 문제점과 궁금증이 생김<br/>\n",
    "지금부터 질문의 대응 방법과 스파크의 조인 수행 방식에 대해 알아보겠음<br/>\n",
    "이 내용에서 최적화와 관련된 몇 가지 힌트를 얻을 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.11.1 복합 데이터 타입의 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불리언을 반환하는 모든 표현식은 조인 표현식으로 간주할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|name            |graduate_program|spark_status   |id |status        |\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|0       |Bill Chambers   |0               |[100]          |100|Contributor   |\n",
      "|1       |Matei Zaharia   |1               |[500, 250, 100]|500|Vice President|\n",
      "|1       |Matei Zaharia   |1               |[500, 250, 100]|250|PMC Member    |\n",
      "|1       |Matei Zaharia   |1               |[500, 250, 100]|100|Contributor   |\n",
      "|2       |Michael Armbrust|1               |[250, 100]     |250|PMC Member    |\n",
      "|2       |Michael Armbrust|1               |[250, 100]     |100|Contributor   |\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\n",
    "    .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.11.2 중복 컬럼명 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인을 수행할 때 가장 까다로운 것 중 하나는 결과 DataFrame에서 중복된 컬럼명을 다루는 것임<br/>\n",
    "DataFrame의 각 컬럼은 스파크 SQL 엔진인 카탈리스트 내에 고유 ID가 있음<br/>\n",
    "고유 ID는 카탈리스트 내부에서만 사용할 수 있으며 직접 참조할 수 있는 값은 아님<br/>\n",
    "그러므로 중복된 컬럼명이 존재하는 DataFrame을 사용할 때는 특정 컬럼을 참조하기 매우 어려움<br/>\n",
    "\n",
    "이런 문제를 일으키는 두 가지 상황은 다음과 같음<br/>\n",
    "* 조인에 사용할 DataFrame의 특정 키가 동일한 이름을 가지며, 키가 제거되지 않도록 조인 표현식에 명시하는 경우\n",
    "* 조인 대상이 아닌 두 개의 컬럼이 동일한 이름을 가진 경우\n",
    "\n",
    "이러한 상황을 설명하기 위해 잘못된 데이터셋을 만들어보겠음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgramDupe: org.apache.spark.sql.DataFrame = [graduate_program: int, degree: string ... 2 more fields]\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = graduate_program)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "\n",
    "val joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\"graduate_program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graduate_program 컬럼을 키로 해서 조인을 수행했음에도 불구하고 두 개의 graduate_program 컬럼이 존재함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |graduate_program|degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+----------------+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |0               |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |1               |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|1               |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 컬럼 중 하나를 참조할 때 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.",
      "  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:363)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.$anonfun$resolveExpressionTopDown$1(Analyzer.scala:1500)",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.innerResolve$1(Analyzer.scala:1502)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.$anonfun$resolveExpressionTopDown$5(Analyzer.scala:1517)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.innerResolve$1(Analyzer.scala:1517)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolveExpressionTopDown(Analyzer.scala:1521)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$12.$anonfun$applyOrElse$98(Analyzer.scala:1698)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$12.applyOrElse(Analyzer.scala:1698)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$12.applyOrElse(Analyzer.scala:1524)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1524)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1353)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:89)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)",
      "  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1462)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1479)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결 방법 1: 다른 조인 표현식 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 이름을 가진 두 개의 키를 사용한다면 가장 쉬운 조치 방법 중 하나는 불리언 형태의 조인 표현식을 문자열이나 시퀀스 형태로 바꾸는 것임<br/>\n",
    "이렇게 하면 조인을 할 때 두 컬럼 중 하나가 자동으로 제거됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|0               |\n",
      "|1               |\n",
      "|1               |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, \"graduate_program\").select(\"graduate_program\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결 방법 2: 조인 후 컬럼 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인 후에 문제가 되는 컬럼을 제거하는 방법도 있음<br/>\n",
    "이 경우에는 원본 DataFrame을 사용해 컬럼을 참조해야 함<br/>\n",
    "조인 시 동일한 키 이름을 사용하거나 원본 DataFrame에 동일한 컬럼명이 존재하는 경우에 사용할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|0               |\n",
      "|1               |\n",
      "|1               |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\"))\n",
    "    .select(\"graduate_program\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|0               |\n",
      "|1               |\n",
      "|1               |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 이렇게 해도 됨\n",
    "person.join(gradProgramDupe, joinExpr).drop(gradProgramDupe.col(\"graduate_program\"))\n",
    "    .select(\"graduate_program\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(graduateProgram, joinExpr).drop(graduateProgram.col(\"id\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법은 스파크의 SQL 분석 프로세스의 특성을 활용함<br/>\n",
    "스파크는 명시적으로 참조된 컬럼을 검증할 필요가 없으므로 스파크 코드 분석 단계를 통과함<br/>\n",
    "위 예제에서 column 함수 대신 col 메서드를 사용한 부분을 주목할 필요가 있음<br/>\n",
    "col 메서드를 사용함으로써 컬럼 고유의 ID로 해당 컬럼을 암시적으로 지정할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결 방법 3: 조인 전 컬럼명 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인 전에 컬럼명을 변경하면 이런 문제를 완전히 회피할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+-------+---------------------+-----------+\n",
      "|id |name            |graduate_program|spark_status   |grad_id|degree |department           |school     |\n",
      "+---+----------------+----------------+---------------+-------+-------+---------------------+-----------+\n",
      "|0  |Bill Chambers   |0               |[100]          |0      |Masters|School of Information|UC Berkeley|\n",
      "|2  |Michael Armbrust|1               |[250, 100]     |1      |Ph.D   |EECS                 |UC Berkeley|\n",
      "|1  |Matei Zaharia   |1               |[500, 250, 100]|1      |Ph.D   |EECS                 |UC Berkeley|\n",
      "+---+----------------+----------------+---------------+-------+-------+---------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gradProgram3: org.apache.spark.sql.DataFrame = [grad_id: int, degree: string ... 2 more fields]\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = grad_id)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "val joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")\n",
    "person.join(gradProgram3, joinExpr).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.12 스파크의 조인 수행 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크가 조인을 수행하는 방식을 이해하기 위해서는 실행에 필요한 두 가지 핵심 전략을 이해해야 함<br/>\n",
    "* 노드 간 네트워크 통신 전략\n",
    "* 노드별 연산 전략 \n",
    "\n",
    "이런 내부 동작은 해결하고자 하는 비즈니스 문제와는 관련이 없을 수도 있음<br/>\n",
    "하지만 스파크 조인 수행 방식을 이해하면 빠르게 완료되는 작업과 절대 완료되지 않는 작업 간의 차이를 알 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.12.1 네트워크 통신 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 조인 시 두 가지 클러스터 통신 방식을 활용함<br/>\n",
    "전체 노드 간 통신을 유발하는 셔플 조인(shuffle join)과 그렇지 않은 브로드캐스트 조인(broadcast join)임<br/>\n",
    "지금은 두 방식의 세부적인 설명은 생략함<br/>\n",
    "이런 내부 최적화 기술은 시간이 흘러 비용 기반 옵티마이저(cost-based optimizer, CBO)가 개선되고 더 나은 통신 전략이 도입되는 경우 바뀔 수 있음<br/>\n",
    "따라서 일반적인 상황에서 정확히 어떤 일이 일어나는지 이해할 수 있도록 고수준 예제를 알아보겠음<br/>\n",
    "그러면 워크로드의 성능을 빠르고 쉽게 최적화하는 방법을 알 수 있음<br/>\n",
    "\n",
    "이제부터는 사용자가 스파크에서 사용하는 테이블의 크기가 아주 크거나 아주 작다고 가정함<br/>\n",
    "물론 실전에서 다루는 테이블의 크기는 다양하므로 중간 크기의 테이블을 활용하는 상황에서 설명과 다르게 동작할 수 있음<br/>\n",
    "하지만 이해를 돕기 위해 동전의 앞뒷면처럼 단순하게 정의하겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 큰 테이블과 큰 테이블 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 큰 테이블을 다른 큰 테이블과 조인하면 셔플 조인이 발생함<br/>\n",
    "\n",
    "셔플 조인은 전체 노드 간 통신이 발생함<br/>\n",
    "그리고 조인에 사용한 특정 키나 키 집합을 어떤 노드가 가졌는지에 따라 해당 노드와 데이터를 공유함<br/>\n",
    "이런 통신 방식 때문에 네트워크는 복잡해지고 많은 자원을 사용함<br/>\n",
    "특히 데이터가 잘 나뉘어 있지 않다면 더 심해짐<br/>\n",
    "\n",
    "셔플 조인 과정은 큰 테이블의 데이터를 다른 큰 테이블의 데이터와 조인하는 과정을 잘 나타냄<br/>\n",
    "예를 들어 사물인터넷 환경에서 매일 수십억 개의 메시지를 수신하고 일별 변경사항을 식별해야 한다면 deviceId, messageType 그리고 data - 1을 하타내는 컬럼을 이용해 조인할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 큰 테이블과 작은 테이블 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테이블이 단일 워커 노드의 메모리 크기에 적합할 정도(메모리 여유 공간 포함)로 충분히 작은 경우 조인 연산을 최적화할 수 있음<br/>\n",
    "큰 테이블 사이의 조인에 사용한 방법도 유용하지만 브로드캐스트 조인이 훨씬 효율적임<br/>\n",
    "이 방법은 작은 DataFrame을 클러스터의 전체 워커 노드에 복제하는 것을 의미함<br/>\n",
    "이렇게 하면 자원을 많이 사용할 것처럼 보임<br/>\n",
    "하지만 조인 프로세스 내내 전체 노드가 통신하는 현상을 방지할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*브로드캐스트 조인은 이전 조인 방식과 마찬가지로 대규모 노드 간 통신이 발생함<br/>\n",
    "하지만 그 이후로는 노드 사이에 추가적인 통신이 발생하지 않음<br/>\n",
    "따라서 모든 단일 노드에서 개별적으로 조인이 수행되므로 CPU가 가장 큰 병목 구간이 됨*<br/>\n",
    "다음 예제와 같이 실행 계획을 살펴보면 스파크가 자동으로 데이터셋을 브로드캐스트 조인으로 설정한 것을 알 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) BroadcastHashJoin [graduate_program#15], [id#34], Inner, BuildLeft, false\n",
      ":- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[2, int, false] as bigint)),false), [id=#686]\n",
      ":  +- LocalTableScan [id#13, name#14, graduate_program#15, spark_status#16]\n",
      "+- *(1) LocalTableScan [id#34, degree#35, department#36, school#37]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "\n",
    "person.join(graduateProgram, joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame API를 사용하면 옵티마이저에서 브로드캐스트 조인을 사용할 수 있도록 힌트를 줄 수 있음<br/>\n",
    "힌트를 주는 방법은 broadcast 함수에 작은 크기의 DataFrame을 인수로 전달하는 것임<br/>\n",
    "다음 예제는 우리가 앞서 보았던 예제와 동일한 실행 계획을 세움<br/>\n",
    "하지만 항상 동일한 실행 계획을 세우는 것은 아님<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) BroadcastHashJoin [graduate_program#15], [id#34], Inner, BuildRight, false\n",
      ":- *(1) LocalTableScan [id#13, name#14, graduate_program#15, spark_status#16]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#702]\n",
      "   +- LocalTableScan [id#34, degree#35, department#36, school#37]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.broadcast\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "\n",
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 단점도 있음<br/>\n",
    "너무 큰 데이터를 브로드캐스트하면 고비용의 수집 연산이 발생하므로 드라이버 노드가 비정상적으로 종료될 수 있음<br/>\n",
    "이러한 현상은 향후 개선되어야 하는 영역임<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아주 작은 테이블 사이의 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 테이블 사이의 조인을 할 때는 스파크가 조인 방식을 결정하도록 내버려두는 것이 제일 좋음<br/>\n",
    "필요한 경우 브로드캐스트 조인을 강제로 지정할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.13 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서는 가장 흔한 사용 사례 중 하나인 조인에 대해 알아보았음<br/>\n",
    "언급하지는 않았지만, 한 가지 중요하게 고려해야 하는 사항이 있음<br/>\n",
    "**조인 전에 데이터를 적절하게 분할하면 셔플이 계획되어 있더라도 동일한 머신에 두 DataFrame의 데이터가 있을 수 있음<br/>\n",
    "따라서 셔플을 피할 수 있고 훨씬 더 효율적으로 실행할 수 있음**<br/>\n",
    "일부 데이터를 실험용으로 사전에 분할해 조인 수행 시 성능이 향상되는지 확인해볼 것<br/>\n",
    "\n",
    "다음 장에서는 스파크의 데이터소스 API에 대해 알아보겠음<br/>\n",
    "데이터소스는 조인 순서를 결정하는 데 부가적인 영향을 미칠 수 있음<br/>\n",
    "일부 조인은 필터 임무를 수행하므로 네트워크의 교환 데이터를 줄여 워크로드의 성능을 쉽게 향상시킬 수 있음<br/>\n",
    "\n",
    "다음 장에서는 최근 몇 장에서 사용한 데이터 처리 방식에서 벗어나 구조적 API를 사용해 데이터를 읽고 쓰는 방법에 대해 자세히 알아보겠음<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
