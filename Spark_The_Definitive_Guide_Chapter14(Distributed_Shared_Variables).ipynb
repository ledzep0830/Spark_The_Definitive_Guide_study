{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.7:4042\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1644049317799)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7214d648\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크의 저수준 API에는 RDD 인터페이스 외에도 두 번째 유형인 '분산형 공유 변수'가 있음<br/>\n",
    "**분산형 공유 변수에는 브로드캐스트 변수와 어큐뮬레이터라는 2개의 타입이 존재함**<br/>\n",
    "클러스터에서 실행할 때 특별한 속성을 가진 사용자 정의 함수(예: RDD나 DataFrame을 다루는 map 함수)에서 이 변수를 사용할 수 있음<br/>\n",
    "특히 **어큐뮬레이터**를 사용하면 모든 태스크의 데이터를 공유 결과에 추가할 수 있음<br/>\n",
    "예를 들어 job의 입력 레코드를 파싱하면서 얼마나 많은 오류가 발생했는지 확인하는 카운터를 구현할 수 있음<br/>\n",
    "반면 **브로드캐스트 변수**를 사용하면 모든 워커 노드에 큰 값을 저장하므로 재전송 없이 많은 스파크 액션에서 재사용할 수 있음<br/>\n",
    "이 장에서는 분산형 공유 변수 타입이 만들어지게 된 계기와 사용 방법에 대해 알아보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.1 브로드캐스트 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "브로드캐스트 변수는 변하지 않는 값을 closure 함수의 변수로 캡슐화하지 않고 클러스터에서 효율적으로 공유하는 방법을 제공함<br/>\n",
    "태스크에서 드라이버 노드의 변수를 사용할 때는 closure 함수 내부에서 단순하게 참조하는 방법을 사용함<br/>\n",
    "하지만 이 방법은 비효율적임<br/>\n",
    "특히 룩업 테이블이나 머신러닝 모델 같은 큰 변수를 사용하는 경우 더 비효율적임<br/>\n",
    "그 이유는 closure 함수에서 변수를 사용할 때 워커 노드에서 여러 번(태스크당 1번) 역직렬화가 일어나기 때문임<br/>\n",
    "게다가 여러 스파크 액션과 job에서 동일한 변수를 사용하면 job을 실행할 때마다 워커로 큰 변수를 재전송함<br/>\n",
    "\n",
    "이런 상황에서는 브로캐스트 변수를 사용해야 함<br/>\n",
    "브로드캐스트 변수는 모든 테스크마다 직렬화하지 않고 클러스터의 모든 머신에 캐시하는 불변성 공유 변수임<br/>\n",
    "executor 메모리 크기에 맞는 조회용 테이블을 전달하고 함수에서 사용하는 것이 대표적인 예임<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클로저의 개념은 [이 블로그 글](https://poiemaweb.com/js-closure)을 참고할 것<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 다음 예제처럼 단어나 값의 목록을 가지고 가지고 있다고 가정하자<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n",
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수 킬로바이트, 메가바이트, 기가바이트 크기를 가진 다른 정보와 함께 단어 목록을 추가해야 할 수도 있음<br/>\n",
    "이 처리는 SQL의 오른쪽 조인에 해당함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supplementalData: scala.collection.immutable.Map[String,Int] = Map(Spark -> 1000, Definitive -> 200, Big -> -300, Simple -> 100)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val supplementalData = Map(\"Spark\" -> 1000, \"Definitive\" -> 200, \"Big\" -> -300, \"Simple\" -> 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 구조체를 스파크에 브로드캐스트할 수 있으며 suppBroadcast 변수를 이용해 참조함<br/>\n",
    "이 값은 불변성이며 액션을 실행할 때 클러스터의 모든 노드에 지연 처리 방식으로 복제됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suppBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] = Broadcast(0)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppBroadcast 변수의 value 메서드를 사용해 위 예제에서 브로드캐스트된 supplementalData 값을 참조할 수 있음<br/>\n",
    "value 메서드는 직렬화된 함수에서 브로드캐스트된 데이터를 직렬화하지 않아도 접근할 수 있음<br/>\n",
    "**스파크는 브로드캐스트 기능을 이용해 데이터를 보다 효율적으로 전송하므로 직렬화와 역직렬화에 대한 부하를 크게 줄일 수 있음**<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: scala.collection.immutable.Map[String,Int] = Map(Spark -> 1000, Definitive -> 200, Big -> -300, Simple -> 100)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 브로드캐스트된 데이터를 사용해 RDD를 변환할 수 있음<br/>\n",
    "다음 예제에서는 맵 연산의 처리 과정에 따라 키-값 쌍 데이터를 생성함<br/>\n",
    "값이 비어 있는 경우 간단하게 0으로 치환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Int)] = Array((Big,-300), (The,0), (Guide,0), (:,0), (Data,0), (Processing,0), (Made,0), (Simple,100), (Definitive,200), (Spark,1000))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))\n",
    "    .sortBy(wordPair => wordPair._2)\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "브로드캐스트 변수를 사용한 방식이 클로저에 담아 전달하는 방식보다 훨씬 더 효율적<br/>\n",
    "물론 데이터의 총량과 executor 수에 따라 다를 수 있으며 아주 작은 데이터(수 KB 정도)를 작은 클러스터에서 돌린다면 크게 차이 나지 않을 수 있음<br/>\n",
    "브로드캐스트 변수에 작은 크기의 dictionary 타입을 사용한다면 큰 부하가 발생하지 않음<br/>\n",
    "하지만 훨씬 큰 크기의 데이터를 사용하는 경우, 전체 태스크에서 데이터를 직렬화하는 데 발생하는 부하가 매우 커질 수 있음<br/>\n",
    "\n",
    "여기서 한 가지 더 주목할 점은 RDD 영역에서 브로드캐스트 변수를 사용했다는 것<br/>\n",
    "그리고 UDF나 Dataset에서도 사용할 수 있으며 동일한 효과를 얻을 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.2 어큐뮬레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어큐뮬레이터는 스파크의 2번째 공유 변수 타입임<br/>\n",
    "어큐뮬레이터는 transformation 내부의 다양한 값을 갱신하는 데 사용함<br/>\n",
    "그리고 내결함성을 보장하면서 효율적인 방식으로 드라이버에 값을 전달할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**어큐뮬레이터는 스파크 클러스터에서 row 단위로 안전하게 값을 갱신할 수 있는 변경 가능한 변수를 제공함**<br/>\n",
    "그리고 디버깅용이나 저수준 집계 생성용으로 사용할 수 있음<br/>\n",
    "ex) 파티션별로 특정 변수의 값을 추적하는 용도로 사용할 수 있으며 시간이 흐를수록 더 유용하게 사용됨<br/>\n",
    "어큐뮬레이터는 결합성과 가환성을 가진 연산을 통해서만 더할 수 있는 변수이므로 병렬 처리 과정에서 효율적으로 사용할 수 있음<br/>\n",
    "어큐뮬레이터는 카운터(맵리듀스의 카운터와 같은)나 합계를 구하는 용도로 사용할 수 있음<br/>\n",
    "스파크는 기본적으로 수치형 어큐뮬레이터를 지원하며 사용자 정의 어큐뮬레이터를 만들어 사용할 수도 있음<br/>\n",
    "\n",
    "어큐뮬레이터의 값은 **액션**을 처리하는 과정에서만 갱신됨<br/>\n",
    "*스파크는 각 태스크에서 어큐뮬레이터를 한 번만 갱신하도록 제어함<br/>\n",
    "따라서 재시작한 태스크는 어큐뮬레이터값을 갱신할 수 없음*<br/>\n",
    "transformation에서 태스크나 job 스테이지를 재처리하는 경우 각 태스크의 갱신 작업이 두 번 이상 적용될 수 있음<br/>\n",
    "\n",
    "어큐뮬레이터는 스파크의 지연 연산 모델에 영향을 주지 않음<br/>\n",
    "어큐뮬레이터가 RDD 처리 중에 갱신되면 RDD 연산이 실제로 수행된 시점, 즉 특정 RDD나 그 RDD의 부모 RDD에 액션을 실행하는 시점에 딱 한 번만 값을 갱신함<br/>\n",
    "따라서 map 함수 같은 지연 처리 형태의 transformation에서 어큐뮬레이터 갱신 작업 수행하는 경우 실제 실행 전까지는 어큐뮬레이터가 갱신되지 않음<br/>\n",
    "\n",
    "어큐뮬레이터의 이름은 선택적으로 지정할 수 있음<br/>\n",
    "이름이 지정된 어큐뮬레이터의 실행 결과는 스파크 UI에 표시되며, 이름이 지정되지 않은 어큐뮬레이터의 경우 스파크 UI에 표시되지 않음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.1 기본 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 만들었던 항공운항 데이터셋에 사용자 정의 집계를 수행하면서 어큐뮬레이터를 실험해보겠음<br/>\n",
    "다음 예제는 RDD API가 아닌 Dataset API를 사용함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "defined class Flight\n",
       "flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "val flights = spark.read\n",
    "    .parquet(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet\")\n",
    "    .as[Flight]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 출발지나 도착지가 중국인 항공편의 수를 구하는 어큐뮬레이터를 생성하자<br/>\n",
    "이런 유형의 집계는 SQL로 처리할 수 있음<br/>\n",
    "하지만 어큐뮬레이터를 사용해 프로그래밍 방식으로 처리해보겠음<br/>\n",
    "다음 예제는 이름이 지정되지 않은 어큐뮬레이터를 생성함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.util.LongAccumulator\n",
       "accUnnamed: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 101, name: None, value: 0)\n",
       "acc: Unit = ()\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.util.LongAccumulator\n",
    "\n",
    "val accUnnamed = new LongAccumulator\n",
    "val acc = spark.sparkContext.register(accUnnamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제에는 이름이 지정된 어큐뮬레이터가 적합함<br/>\n",
    "어큐뮬레이터를 만드는 가장 간단한 방법은 SparkContext를 사용하는 것임<br/>\n",
    "아니면 직접 어큐뮬레이터를 생성하고 이름을 붙여 등록할 수도 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accChina: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 103, name: Some(China), value: 0)\n",
       "accChina2: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 102, name: Some(China), value: 0)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accChina = new LongAccumulator\n",
    "val accChina2 = spark.sparkContext.longAccumulator(\"China\")\n",
    "\n",
    "spark.sparkContext.register(accChina, \"China\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수의 파라미터로 문자열 값을 전달하거나 register 함수의 두 번째 파라미터를 사용해 이름을 지정할 수 있음<br/>\n",
    "이름이 지정된 어큐뮬레이터는 실행 결과를 스파크 UI에서 확인할 수 있으며 이름이 지정되지 않았다면 스파크 UI에서 확인할 수 없음<br/>\n",
    "\n",
    "다음은 어큐뮬레이터에 값을 더하는 방법을 정의하는 단계임<br/>\n",
    "다음 예제의 함수는 직관적인 형태로 구성되어 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accChinaFunc: (flight_row: Flight)Unit\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accChinaFunc(flight_row: Flight) = {\n",
    "    val destination = flight_row.DEST_COUNTRY_NAME\n",
    "    val origin = flight_row.ORIGIN_COUNTRY_NAME\n",
    "    \n",
    "    if (destination == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "    if (origin == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 foreach 메서드를 사용해 항공운항 데이터셋의 전체 로우를 처리해보겠음<br/>\n",
    "이렇게 하는 이유는 foreach 메서드가 액션이고, 스파크는 액션에서만 어큐뮬레이터의 실행을 보장하기 때문<br/>\n",
    "foreach 메서드는 입력 DataFrame의 매 로우마다 함수를 한 번씩 적용해 어큐뮬레이터 값을 증가시킴<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.foreach(flight_row => accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 연산은 상당히 빨리 종료됨<br/>\n",
    "스파크 UI에서 executor 단위로 어큐뮬레이터 값을 확인할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어큐뮬레이터 값을 프로그래밍 방식으로 조회할 수도 있음<br/>\n",
    "이 경우 value 속성을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 953\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accChina.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.2 사용자 정의 어큐뮬레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 몇 가지 기본 어큐뮬레이터를 제공함<br/>\n",
    "하지만 때에 따라 사용자 정의 어큐뮬레이터가 필요할 수도 있음<br/>\n",
    "**어큐뮬레이터를 직접 정의하려면 AccumulatorV2 클래스를 상속받아야 함**<br/>\n",
    "다음 예제와 같이 구현해야 하는 추상 메서드가 몇 가지 있음<br/>\n",
    "다음은 어큐뮬레이터에 짝수값만 더하는 예제임<br/>\n",
    "아주 간단하지만 사용자 정의 어큐뮬레이터를 구현하는 과정이 얼마나 쉬운지 알 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.util.AccumulatorV2\n",
       "arr: scala.collection.mutable.ArrayBuffer[BigInt] = ArrayBuffer()\n",
       "defined class EvenAccumulator\n",
       "acc: EvenAccumulator = EvenAccumulator(id: 290, name: Some(evenAcc), value: 0)\n",
       "newAcc: Unit = ()\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "  private var num:BigInt = 0\n",
    "  def reset(): Unit = {\n",
    "    this.num = 0\n",
    "  }\n",
    "  def add(intValue: BigInt): Unit = {\n",
    "    if (intValue % 2 == 0) {\n",
    "        this.num += intValue\n",
    "    }\n",
    "  }\n",
    "  def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {\n",
    "    this.num += other.value\n",
    "  }\n",
    "  def value():BigInt = {\n",
    "    this.num\n",
    "  }\n",
    "  def copy(): AccumulatorV2[BigInt,BigInt] = {\n",
    "    new EvenAccumulator\n",
    "  }\n",
    "  def isZero():Boolean = {\n",
    "    this.num == 0\n",
    "  }\n",
    "}\n",
    "val acc = new EvenAccumulator\n",
    "val newAcc = sc.register(acc, \"evenAcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: BigInt = 0\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value // 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Exception while getting task result: java.io.IOException: java.lang.ClassNotFoundException: EvenAccumulator",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: java.io.IOException: java.lang.ClassNotFoundException: EvenAccumulator",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)",
      "  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1012)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1010)",
      "  at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:2887)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:3676)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3674)",
      "  at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2887)",
      "  ... 39 elided",
      ""
     ]
    }
   ],
   "source": [
    "flights.foreach(flight_row => acc.add(flight_row.count))\n",
    "acc.value // 31390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드에서 에러가 나는 이유를 모르겠음<br/>\n",
    "\n",
    "에러 메시지에 java.io.IOException: java.lang.ClassNotFoundException: EvenAccumulator 문구가 있으므로 클래스를 정의한 코드와 커맨드를 함께 실행하는 경우 아래와 같은 다른 에러 메시지가 뜸 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Exception while getting task result: java.io.IOException: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.BoundReference.accessor of type scala.Function2 in instance of org.apache.spark.sql.catalyst.expressions.BoundReference",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: java.io.IOException: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.BoundReference.accessor of type scala.Function2 in instance of org.apache.spark.sql.catalyst.expressions.BoundReference",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)",
      "  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1012)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1010)",
      "  at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:2887)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:3676)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3674)",
      "  at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2887)",
      "  ... 30 elided",
      ""
     ]
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "  private var num:BigInt = 0\n",
    "  def reset(): Unit = {\n",
    "    this.num = 0\n",
    "  }\n",
    "  def add(intValue: BigInt): Unit = {\n",
    "    if (intValue % 2 == 0) {\n",
    "        this.num += intValue\n",
    "    }\n",
    "  }\n",
    "  def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {\n",
    "    this.num += other.value\n",
    "  }\n",
    "  def value():BigInt = {\n",
    "    this.num\n",
    "  }\n",
    "  def copy(): AccumulatorV2[BigInt,BigInt] = {\n",
    "    new EvenAccumulator\n",
    "  }\n",
    "  def isZero():Boolean = {\n",
    "    this.num == 0\n",
    "  }\n",
    "}\n",
    "val acc = new EvenAccumulator\n",
    "val newAcc = sc.register(acc, \"evenAcc\")\n",
    "\n",
    "flights.foreach(flight_row => acc.add(flight_row.count))\n",
    "acc.value // 31390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나중에 에러의 원인을 찾아내보겠다<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.3 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서는 분산형 공유 변수를 알아보았음<br/>\n",
    "**분산형 공유 변수는 디버깅이나 최적화 작업에 유용한 도구임**<br/>\n",
    "다음 장에서는 분산형 공유 변수가 언제 도움이 되는지 이해하기 위해 클러스터 환경에서 스파크가 동작하는 방식에 대해 알아보겠음<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
