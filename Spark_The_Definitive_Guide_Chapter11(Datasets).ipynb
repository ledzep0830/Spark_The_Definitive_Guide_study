{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.7:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1642841918971)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1046b641\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset은 구조적 API의 기본 데이터 타입임<br/>\n",
    "앞에서 이미 DataFrame을 다루었음<br/>\n",
    "DataFrame은 Row 타입의 Dataset이며 스파크가 지원하는 다양한 언어에서 사용할 수 있음<br/>\n",
    "(스파크 소스 코드 중 org/apache/spark/sql/package.scala 파일을 보면 type DataFrame = Dataset\\[Row\\]로 선언되어 있음)<br/>\n",
    "**Dataset은 자바 가상 머신을 사용하는 언어인 스칼라와 자바에서만 사용 가능함**<br/>\n",
    "Dataset을 사용해 데이터셋의 각 로우를 구성하는 객체를 정의함<br/>\n",
    "스칼라에서는 스키마가 정의된 케이스 클래스 객체를 사용해 Dataset을 정의함<br/>\n",
    "자바에서는 자바 빈 객체를 사용해 Dataset을 정의함<br/>\n",
    "\n",
    "스파크는 StringType, BigIntType, StructType과 같은 다양한 데이터 타입을 제공함<br/>\n",
    "또한 스파크가 지원하는 다양한 언어의 String, Integer 그리고 Double과 같은 데이터 타입을 스파크의 특정 데이터 타입으로 매핑할 수 있음<br/>\n",
    "DataFrame API를 사용할 때 String이나 Integer 데이터 타입의 객체를 생성하지는 않지만 스파크는 Row 객체를 변환해 데이터를 처리함<br/>\n",
    "**사실 스칼라나 자바를 사용할 때 모든 DataFrame은 Row 타입의 Dataset을 의미함**<br/>\n",
    "도메인별 특정 객체를 효과적으로 지원하기 위해 encoder라고 부르는 특수 개념이 필요함<br/>\n",
    "encoder는 도메인별 특정 객체 T를 스파크의 내부 데이터 타입으로 매핑하는 시스템을 의미함<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 name(string)과 age(int) 두 개의 필드를 가진 Person 클래스가 있다고 가정하자<br/>\n",
    "encoder는 런타임 환경에서 Person 객체를 바이너리 구조로 직렬화하는 코드를 생성하도록 스파크에 지시함<br/>\n",
    "DataFrame이나 '표준' 구조적 API를 사용한다면 Row 타입을 직렬화된 바이너리 구조로 변환함<br/>\n",
    "도메인에 특화된 객체를 만들어 사용하려면 스칼라의 케이스 클래스(case class) 또는 자바의 자바빈(JavaBean) 형태로 사용자 정의 데이터 타입을 정의해야 함<br/>\n",
    "스파크에서는 Row 타입 대신 사용자가 정의한 데이터 타입을 분산 방식으로 다룰 수 있음<br/>\n",
    "\n",
    "**Dataset API를 사용한다면 스파크는 데이터셋에 접근할 때마다 Row 포맷이 아닌 사용자 정의 데이터 타입으로 변환함<br/>\n",
    "이 변환 작업은 느리긴 하지만 사용자에게 더 많은 유연성을 제공할 수 있음<br/>\n",
    "사용자 정의 데이터 타입을 사용하면 성능이 나빠지게 됨<br/>\n",
    "파이썬으로 만든 사용자 정의 함수와 비슷한 상황으로 볼 수 있으나 자릿수가 다를 정도로 사용자 정의 함수가 훨씬 더 느림<br/>\n",
    "그 이유는 프로그래밍 언어를 전환하는 것이 사용자 정의 데이터 타입을 사용하는 것보다 훨씬 더 느리기 때문**<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1 Dataset을 사용할 시기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 사용하면 성능이 떨어진다는데 사용할 필요가 있을까?<br/>\n",
    "Dataset을 사용해야 하는 2가지 이유<br/>\n",
    "* DataFrame 기능만으로 수행할 연산을 표현할 수 없는 경우\n",
    "* 성능 저하를 감수하더라도 타입 안정성(type-safety)을 가진 데이터 타입을 사용하고 싶은 경우\n",
    "\n",
    "Dataset을 사용해야 하는 이유를 자세히 알아보겠음<br/>\n",
    "구조적 API를 사용해 표현할 수 없는 몇 가지 작업이 있음<br/>\n",
    "흔하진 않지만, 복잡한 비즈니스 로직을 SQL이나 DataFrame 대신 단일 함수로 인코딩해야 하는 경우가 있음<br/>\n",
    "이런 경우 Dataset을 사용하는 것이 적합함<br/>\n",
    "또한 Dataset API는 타입 안정성이 있음<br/>\n",
    "예를 들어 두 문자열을 사용해 뺄셈 연산을 하는 것처럼 데이터 타입이 유효하지 않은 작업은 런타임이 아닌 컴파일 타임에 요류가 발생함<br/>\n",
    "\n",
    "단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 transformation을 재사용하려면 Dataset을 사용하는 것이 적합함<br/>\n",
    "*스파크의 API는 스칼라 Sequence 타입의 API가 일부 반영되어 있지만 분산 방식으로 동작함*<br/>\n",
    "결국 Dataset을 사용하는 장점 중 하나는 로컬과 분산 환경의 워크로드에서 재사용할 수 있다는 것임<br/>\n",
    "케이스 클래스로 구현된 데이터 타입을 사용해 모든 데이터와 transformation을 정의하면 재사용할 수 있음<br/>\n",
    "또한 올바른 클래스와 데이터 타입이 지정된 DataFrame을 로컬 디스크에 저장하면 다음 처리 과정에서 사용할 수 있어 더 쉽게 데이터를 다룰 수 있음<br/>\n",
    "\n",
    "더 적합한 워크로드를 만들기 위해 DataFrame과 Dataset을 동시에 사용해야 할 때가 있음<br/>\n",
    "하지만 성능과 타입 안정성 중 하나는 반드시 희생할 수밖에 없음<br/>\n",
    "*이러한 방식은 대량의 DataFrame 기반의 ETL transformation의 마지막 단계에서 사용할 수 있음<br/>\n",
    "예를 들어 드라이버로 데이터를 수집해 단일 노드의 라이브러리로 수집된 데이터를 처리하는 경우*<br/>\n",
    "반대로 transformation의 첫 번째 단계에서 사용할 수도 있음 (예를 들어 스파크 SQL에서 필터링 전에 로우 단위로 데이터를 파싱하는 경우)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 Dataset 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset을 생성하는 것은 수동 작업이므로 정의할 스키마를 미리 알고 있어야 함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.1 자바: Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자바 인코더는 매우 간단함<br/>\n",
    "데이터 타입 클래스를 정의한 다음 DataFrame(Dataset\\< Row\\> 타입의)에 지정해 인코딩할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "// 자바 코드\n",
    "\n",
    "import org.apache.spark.sql.Encoders;\n",
    "\n",
    "public class Flight implements Serializable{\n",
    "    String DEST_COUNTRY_NAME;\n",
    "    String ORIGIN_COUNTRY_NAME;\n",
    "    Long DEST_COUNTRY_NAME;\n",
    "}\n",
    "\n",
    "Dataset<Flight> flights = spark.read\n",
    "    .parquet(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "    .as(Encoders.bean(Flight.class));\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.2 스칼라: 케이스 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라에서 Dataset을 생성하려면 스칼라 case class 구문을 사용해 데이터 타입을 정의해야 함<br/>\n",
    "케이스 클래스는 다음과 같은 특징을 가진 정규 클래스(regular class)임<br/>\n",
    "* 불변성\n",
    "* 패턴 매칭으로 분해 가능\n",
    "* 참조값 대신 클래스 구조를 기반으로 비교\n",
    "* 사용하기 쉽고 다루기 편함\n",
    "\n",
    "이러한 특징으로 케이스 클래스를 판별할 수 있으므로 데이터 분석 시 유용함<br/>\n",
    "\n",
    "스칼라 문서는 케이스 클래스를 다음과 같이 설명함<br/>\n",
    "* 불변성이므로 객체들이 언제 어디서 변경되었는지 추적할 필요가 없음<br/>\n",
    "* 패턴 매칭은 로직 분기를 단순화해 버그를 줄이고 가독성을 좋게 만듦<br/>\n",
    "\n",
    "Dataset을 생성하기 위해 예제 데이터셋 중 하나를 case class로 정의함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 데이터셋의 레코드를 표현할 case class를 정의했음<br/>\n",
    "즉, Flight 데이터 타입의 Dataset을 생성했음<br/>\n",
    "Flight 데이터 타입은 스키마만 정의되어 있을 뿐 아무런 메서드도 정의되어 있지 않음<br/>\n",
    "데이터를 읽으면 DataFrame이 반환됨<br/>\n",
    "그리고 as 메서드를 사용해 Flights 데이터 타입으로 변환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsDF = spark.read\n",
    "    .parquet(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet\")\n",
    "val flights = flightsDF.as[Flight]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 액션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset과 DataFrame에 collect, take 그리고 count 같은 액션을 적용할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 케이스 클래스에 실제로 접근할 때 어떠한 데이터 타입도 필요하지 않다는 사실을 알고 있어야 함<br/>\n",
    "case class의 속성명을 지정하면 속성에 맞는 값과 데이터 타입 모두를 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: String = United States\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.first.DEST_COUNTRY_NAME // United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset의 트랜스포메이션은 DataFrame과 동일함<br/>\n",
    "DataFrame의 모든 트랜스포메이션은 Dataset에서 사용할 수 있음<br/>\n",
    "\n",
    "**Dataset을 사용하면 원형의 JVM 데이터 타입을 다루기 때문에 DataFrame만 사용해서 트랜스포메이션을 수행하는 것보다 좀 더 복잡하고 강력한 데이터 타입으로 트랜스포메이션을 사용할 수 있음**<br/>\n",
    "원형 객체를 다루는 방법을 설명하기 위해 이전 예제에서 만든 Dataset에 필터를 적용해보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.1 필터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flight 클래스를 파라미터로 사용해 출발지와 도착지가 동일한지를 나타내는 불리언값을 반환하는 함수를 만들어보겠음<br/>\n",
    "이 함수는 사용자 정의 함수(스파크 SQL은 예제와 같은 방식으로 사용자 정의 함수를 정의함)가 아닌 일반 함수임<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TIP*<br/>\n",
    "다음 예제에서는 필터가 정의된 **함수**를 생성함<br/>\n",
    "이 방식은 지금까지 했던 방식과는 다르므로 매우 중요함<br/>\n",
    "스파크는 정의된 함수를 사용해 모든 로우를 평가하고, 따라서 매우 많은 자원을 사용함<br/>\n",
    "그러므로 단순 필터라면 SQL 표현식을 사용하는 것이 좋음<br/>\n",
    "SQL 표현식을 사용하면 데이터 필터링 비용이 크게 줄어들 뿐만 아니라 다음 처리 과정에서 Dataset으로 데이터를 다룰 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originIsDestination: (flight_row: Flight)Boolean\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "    return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[Flight] = Array(Flight(United States,United States,348113))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.collect().filter(flight_row => originIsDestination(flight_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4.2 매핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필터링은 단순한 트랜스포메이션임<br/>\n",
    "때로는 특정 값을 다른 값으로 매핑해야 함<br/>\n",
    "이전 예제에서 함수를 정의하면서 이미 매핑 작업을 해봤음<br/>\n",
    "정의한 함수는 Flight 데이터 타입을 입력으로 사용해 불리언값을 반환함<br/>\n",
    "하지만 실무에서는 값을 추출하거나 값을 비교하는 것과 같은 정교한 처리가 필요함<br/>\n",
    "\n",
    "Dataset을 다루는 가장 간단한 예제는 로우의 특정 컬럼값을 추출하는 것임<br/>\n",
    "DataFrame에 매핑 작업을 수행하는 것은 Datset의 select 메서드를 사용하는 것과 같음<br/>\n",
    "다음은 목적지 컬럼을 추출하는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destinations: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val destinations = flights.map(f => f.DEST_COUNTRY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 String 데이터 타입의 Dataset을 반환함<br/>\n",
    "스파크는 결과로 반환할 JVM 데이터 타입을 알고 있기 때문에 컴파일 타임에 데이터 타입의 유효성을 검사할 수 있음<br/>\n",
    "\n",
    "드라이버는 결과값을 모아 문자열 타입의 배열로 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (192.168.0.7 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (192.168.0.7 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  ... 36 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "val localDestinations = destinations.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*위 에러는 왜 생기는 것인지 모르겠음*<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.5 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조인은 DataFrame에서와 마찬가지로 Dataset에도 동일하게 적용됨<br/>\n",
    "하지만 Dataset은 joinWith처럼 정교한 메서드를 제공함<br/>\n",
    "joinWith 메서드는 co-group(RDD 용어)과 거의 유사하며 Dataset 안쪽에 다른 두 개의 중첩된 Dataset으로 구성됨<br/>\n",
    "각 컬럼은 단일 Dataset이므로 Dataset 객체를 컬럼처럼 다룰 수 있음<br/>\n",
    "그러므로 조인 수행 시 더 많은 정보를 유지할 수 있으며 고급 맵이나 필터처럼 정교하게 데이터를 다룰 수 있음<br/>\n",
    "\n",
    "joinWith 메서드를 설명하기 위해 가짜 항공운항 메타데이터 Dataset을 생성함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "defined class FlightMetadata\n",
       "flightsMeta: org.apache.spark.sql.Dataset[FlightMetadata] = [count: bigint, randomData: bigint]\n",
       "flights2: org.apache.spark.sql.Dataset[(Flight, FlightMetadata)] = [_1: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field>, _2: struct<count: bigint, randomData: bigint>]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._ // 이 라인을 추가해야 실행됨\n",
    "\n",
    "case class FlightMetadata(count: BigInt, randomData: BigInt)\n",
    "\n",
    "val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))\n",
    "  .withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\")\n",
    "  .as[FlightMetadata]\n",
    "\n",
    "val flights2 = flights\n",
    "  .joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 로우는 Flight와 FlightMetadata로 이루어진 일종의 키-값 형태의 Dataset을 반환함<br/>\n",
    "Dataset이나 복합 데이터 타입의 DataFrame으로 데이터를 조회할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------+\n",
      "|_1                                  |_2                      |\n",
      "+------------------------------------+------------------------+\n",
      "|{United States, Uganda, 1}          |{1, 2795269918196455456}|\n",
      "|{United States, French Guiana, 1}   |{1, 2795269918196455456}|\n",
      "|{Bulgaria, United States, 1}        |{1, 2795269918196455456}|\n",
      "|{United States, Slovakia, 1}        |{1, 2795269918196455456}|\n",
      "|{United States, Cameroon, 1}        |{1, 2795269918196455456}|\n",
      "|{United States, Burkina Faso, 1}    |{1, 2795269918196455456}|\n",
      "|{United States, Indonesia, 1}       |{1, 2795269918196455456}|\n",
      "|{Vietnam, United States, 1}         |{1, 2795269918196455456}|\n",
      "|{United States, Papua New Guinea, 1}|{1, 2795269918196455456}|\n",
      "|{United States, Bahrain, 1}         |{1, 2795269918196455456}|\n",
      "|{United States, Gabon, 1}           |{1, 2795269918196455456}|\n",
      "|{United States, Liberia, 1}         |{1, 2795269918196455456}|\n",
      "|{Malaysia, United States, 1}        |{1, 2795269918196455456}|\n",
      "|{United States, Serbia, 1}          |{1, 2795269918196455456}|\n",
      "|{United States, Azerbaijan, 1}      |{1, 2795269918196455456}|\n",
      "|{United States, Estonia, 1}         |{1, 2795269918196455456}|\n",
      "|{United States, Vietnam, 1}         |{1, 2795269918196455456}|\n",
      "|{Liberia, United States, 1}         |{1, 2795269918196455456}|\n",
      "|{Azerbaijan, United States, 1}      |{1, 2795269918196455456}|\n",
      "|{United States, Algeria, 1}         |{1, 2795269918196455456}|\n",
      "+------------------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|         Bulgaria|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|          Vietnam|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|         Malaysia|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|          Liberia|\n",
      "|       Azerbaijan|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights2.selectExpr(\"_1.DEST_COUNTRY_NAME\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드라이버로 데이터를 모은 다음 결과를 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[(Flight, FlightMetadata)] = Array((Flight(United States,Uganda,1),FlightMetadata(1,-5337248526406846490)), (Flight(United States,French Guiana,1),FlightMetadata(1,-5337248526406846490)))\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 조인 역시 잘 동작함<br/>\n",
    "하지만 DataFrame을 반환하므로 JVM 데이터 타입 정보를 모두 잃게 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights.join(flightsMeta, Seq(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 정보를 다시 얻으려면 다른 Dataset을 정의해야 함<br/>\n",
    "DataFrame과 Dataset을 조인하는 것은 아무런 문제가 되지 않으며 최종적으로 동일한 결과를 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.6 그룹화와 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그룹화와 집계는 이전 장에서 보았던 것과 동일한 표준을 따르므로 groupBy, rollup 그리고 cube 메서드를 여전히 사용할 수 있음<br/>\n",
    "하지만 Dataset 대신 DataFrame을 반환하기 때문에 데이터 타입 정보를 잃게 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 타입 정보를 잃는 것은 그다지 큰 문제는 아니지만, 이를 유지할 수 있는 그룹화와 집계 방법이 있음<br/>\n",
    "한 가지 예로 groupByKey 메서드는 Dataset의 특정 키를 기준으로 그룹화하고 형식화된 Dataset을 반환함<br/>\n",
    "하지만 이 함수는 컬럼명 대신 함수를 파라미터로 사용해야 함<br/>\n",
    "따라서 다음 예제와 같이 훨씬 더 정교한 그룹화 함수를 사용할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupByKey 메서드의 파라미터로 함수를 사용함으로써 유연성을 얻을 수 있음<br/>\n",
    "하지만 스파크는 함수와 JVM 데이터 타입을 최적화할 수 없으므로 trade-off가 발생함<br/>\n",
    "이로 인해 성능 차이가 발생할 수 있으며 실행 계획으로 그 이유를 확인할 수 있음<br/>\n",
    "다음 예제는 groupByKey 메서드를 사용해 DataFrame에 새로운 컬럼을 추가한 다음 그룹화를 수행함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[value#309], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(value#309, 200), ENSURE_REQUIREMENTS, [id=#608]\n",
      "   +- *(2) HashAggregate(keys=[value#309], functions=[partial_count(1)])\n",
      "      +- *(2) Project [value#309]\n",
      "         +- AppendColumns $line82.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4094/798789902@1be80bc4, newInstance(class $line8.$read$$iw$$iw$Flight), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#309]\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [DEST_COUNTRY_NAME#0,ORIGIN_COUNTRY_NAME#1,count#2L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/hj/Downloads/Spark-The-Definitive-Guide/data/flight-data/parquet/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset의 키를 이용해 그룹화를 수행한 다음 결과를 키-값 형태로 함수에 전달해 원시 객체 형태로 그룹화된 데이터를 다룰 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 24) (192.168.0.7 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 24) (192.168.0.7 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)",
      "\tat org.apache.spark.sql.execution.ObjectOperator$.$anonfun$deserializeRowToObject$1(objects.scala:148)",
      "\tat org.apache.spark.sql.execution.AppendColumnsExec.$anonfun$doExecute$13(objects.scala:331)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:156)",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  ... 46 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.NewInstance_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.ObjectOperator$.$anonfun$deserializeRowToObject$1(objects.scala:148)",
      "  at org.apache.spark.sql.execution.AppendColumnsExec.$anonfun$doExecute$13(objects.scala:331)",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:156)",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "def grpSum(countryName:String, values: Iterator[Flight]) = {\n",
    "  values.dropWhile(_.count < 5).map(x => (countryName, x))\n",
    "}\n",
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*위와 같은 에러인데 마찬가지로 왜 생기는지 모르겠음*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#0], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#0, 200), ENSURE_REQUIREMENTS, [id=#816]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#0], functions=[partial_count(1)])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [DEST_COUNTRY_NAME#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/hj/Downloads/Spark-The-Definitive-Guide/data/flight-data/parquet/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count().explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupByKey 메서드는 동일한 결과를 반환하지만 데이터 스캔 직후에 집계를 수행하는 groupBy 메서드에 비해 더 비싼 처리라는 것을 실행 계획으로 알 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.7 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서는 Dataset의 기초와 Dataset 사용이 적합한 경우를 예제와 함께 알아보았음<br/>\n",
    "다시 말해, Dataset을 사용하기 위해 기본적으로 알아야 하는 모든 내용과 Dataset 사용 방법에 대해 알아보았음<br/>\n",
    "Dataset은 고수준의 구조적 API와 다음 장의 주제인 저수준 RDD API가 조합된 형태라고 생각하면 쉽게 이해할 수 있음<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
