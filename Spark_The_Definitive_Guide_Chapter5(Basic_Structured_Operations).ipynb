{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4장에서는 구조적 API의 핵심 추상화 개념을 알아보았음<br/>\n",
    "이 장에서는 DataFrame과 DataFrame의 데이터 다루는 기능을 소개하고, 특히 DataFrame의 기본 기능을 중점적으로 다룸<br/>\n",
    "DataFrame을 사용한 집계, 윈도우 함수, 조인 등의 내용은 7장과 8장에서 자세히 알아볼 것임<br/>\n",
    "\n",
    "DataFrame은 Row 타입의 **레코드**(테이블의 로우 같은)와 각 레코드에 수행할 연산 표현식을 나타내는 여러 **컬럼**으로 구성됨<br/>\n",
    "**스키마**는 각 컬럼명과 데이터 타입을 정의함<br/>\n",
    "DataFrame의 **파티셔닝**은 DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의함<br/>\n",
    "**파티셔닝 스키마**는 파티션을 배치하는 방법을 정의함<br/>\n",
    "파티셔닝의 분할 기준은 특정 컬럼이나 비결정론적(nondeterministic) 값을 기반으로 설정할 수 있음<br/>\n",
    "\n",
    "우선 DataFrame을 생성함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.7:4042\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1640957232996)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@a21728b\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"json\")\n",
    "    .load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame은 컬럼을 가지며 스키마로 컬럼을 정의함<br/>\n",
    "앞 예제에서 만든 DataFrame의 스키마를 살펴보겠음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스키마는 관련된 모든 것을 하나로 묶는 역할을 함<br/>\n",
    "스키마에 대해 자세히 알아보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 스키마\n",
    "스키마는 DataFrame의 컬럼명과 데이터 타입을 정의함<br/>\n",
    "데이터 소스에서 스키마를 얻거나 직접 정의할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CAUTION*<br/>\n",
    "데이터를 읽기 전에 스키마를 정의해야 하는지 여부는 상황에 따라 달라짐<br/>\n",
    "비정형 분석(ad-hoc analysis)에서는 스키마-온-리드가 대부분 잘 동작함 (단, csv나 json 같은 일반 텍스트 파일을 사용하면 다소 느릴 수 있음)<br/>\n",
    "하지만 Long 데이터 타입을 Integer 데이터 타입으로 잘못 인식하는 등 정밀도 문제가 발생할 수 있음<br/>\n",
    "따라서 운영 환경에서 추출(Extract), 변환(Transform), 적재(Load)를 수행하는 ETL 작업에 스파크를 사용한다면 직접 스키마를 정의해야 함<br/>\n",
    "ETL 작업 중에 데이터 타입을 알기 힘든 csv나 json 등의 데이터 소스를 사용하는 경우, 스키마 추론 과정에서 읽어들인 샘플 데이터의 타입에 따라 스키마를 결정해버릴 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스키마는 *여러 개의 StructField 타입 필드로 구성된 StructType 객체*임<br/>\n",
    "StructField는 이름, 데이터 타입, 컬럼이 값이 없거나 null일 수 있는지 지정하는 Boolean 값을 가짐<br/>\n",
    "필요한 경우 컬럼과 관련된 메타데이터를 지정할 수도 있음<br/>\n",
    "메타데이터는 해당 컬럼과 관련된 정보이며 스파크의 머신러닝 라이브러리에서 사용함<br/>\n",
    "\n",
    "스키마는 복합 데이터 타입인 StructType을 가질 수 있음<br/>\n",
    "복합 데이터 타입은 6장에서 자세히 설명함<br/>\n",
    "스파크는 런타임에 데이터 타입이 스키마의 데이터 타입과 일치하지 않으면 오류를 발생시킴<br/>\n",
    "다음 코드는 DataFrame에 스키마를 만들고 적용하는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "import org.apache.spark.sql.types.Metadata\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata\n",
    "\n",
    "val myManualSchema = StructType(Array(\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "    StructField(\"count\", LongType, false,\n",
    "        Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\"))\n",
    "))\n",
    "\n",
    "val df = spark.read.format(\"json\").schema(myManualSchema)\n",
    "    .load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 자체 데이터 타입 정보를 사용하므로 프로그래밍 언어의 데이터 타입을 스파크의 데이터 타입으로 설정할 수 없음<br/>\n",
    "다음 절에서는 스키마에 정의하는 컬럼에 대해 알아보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 컬럼과 표현식\n",
    "스파크의 컬럼은 스프레드시트, R의 dataframe, Pandas의 DataFrame 컬럼과 유사함<br/>\n",
    "사용자는 **표현식**으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있음<br/>\n",
    "\n",
    "스파크의 컬럼은 *표현식을 사용해 레코드 단위로 계산한 값을 단순하게 나타내는 논리적인 구조*임<br/>\n",
    "따라서 컬럼의 실제 값을 얻으려면 로우가 필요하고, 로우를 얻으려면 DataFrame이 필요함<br/>\n",
    "DataFrame을 통하지 않으면 외부에서 컬럼에 접근할 수 없음<br/>\n",
    "컬럼 내용을 수정하려면 반드시 DataFrame의 스파크 transformation을 사용해야 함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 컬럼\n",
    "컬럼을 생성하고 참조할 수 있는 여러 가지 방법이 있지만, col 함수나 column 함수를 사용하는 것이 가장 간단함<br/>\n",
    "이들 함수는 컬럼명을 인수로 받음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, column}\n",
       "res3: org.apache.spark.sql.Column = someColumnName\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, column}\n",
    "\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 책에서는 col 함수를 계속해서 사용함<br/>\n",
    "컬럼이 DataFrame에 있을지 없을지는 알 수 없음<br/>\n",
    "컬럼은 컬럼명을 **카탈로그**에 저장된 정보와 비교하기 전까지 **미확인** 상태로 남음<br/>\n",
    "4장에서 알아본 것처럼 **분석기**가 동작하는 단계에서 컬럼과 테이블을 분석함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명시적 컬럼 참조\n",
    "DataFrame의 컬럼은 col 메서드로 참조함<br/>\n",
    "col 메서드는 조인 시 유용함<br/>\n",
    "예를 들어 DataFrame의 어떤 컬럼을 다른 DataFrame의 조인 대상 컬럼에서 참조하기 위해 col 메서드를 사용함<br/>\n",
    "조인은 8장에서 자세히 알아보겠음<br/>\n",
    "col 메서드를 사용해 명시적으로 컬럼을 정의하면 스파크는 분석기 실행 단계에서 컬럼 확인 절차를 생략함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.Column = count\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.col(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표현식으로 컬럼 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Column = ((((someCol + 5) * 200) - 6) < otherCol)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "res6: org.apache.spark.sql.Column = ((((someCol + 5) * 200) - 6) < otherCol)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 컬럼에 접근하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 레코드와 로우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.Row = [United States,Romania,15]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 로우 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "myRow: org.apache.spark.sql.Row = [Hello,null,1,false]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val myRow = Row(\"Hello\", null, 1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Any = Hello\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow(0) // Any 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: String = Hello\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow(0).asInstanceOf[String] // String 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: String = Hello\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow.getString(0) // String 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Int = 1\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow.getInt(2) // Int 타입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset API를 이용하면 JVM 객체를 가진 데이터셋을 얻을 수 있음<br/>\n",
    "Dataset은 11장에서 자세히 알아보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 DataFrame의 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 DataFrame의 핵심 영역을 간단히 살펴봤음 <br/>\n",
    "이어서 DataFrame을 다루는 방법을 알아보겠음<br/>\n",
    "DataFrame을 다루는 방법은 몇 가지 주요 작업으로 나눌 수 있음<br/>\n",
    "* 로우나 컬럼 추가\n",
    "* 로우나 컬럼 제거\n",
    "* 로우를 컬럼으로 변환하거나, 그 반대로 변환\n",
    "* 컬럼값을 기준으로 로우 순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.1 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"json\")\n",
    "    .load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\n",
       "myRows: Seq[org.apache.spark.sql.Row] = List([Hello,null,1])\n",
       "myRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[23] at parallelize at <console>:40\n",
       "myDf: org.apache.spark.sql.DataFrame = [some: string, col: string ... 1 more field]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "    new StructField(\"some\", StringType, true),\n",
    "    new StructField(\"col\", StringType, true),\n",
    "    new StructField(\"names\", LongType, false)))\n",
    "\n",
    "val myRows = Seq(Row(\"Hello\", null, 1L))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val myDf = spark.createDataFrame(myRDD, myManualSchema)\n",
    "\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*<br/>\n",
    "스칼라 버전의 스파크 콘솔을 사용하는 경우 Seq 데이터 타입에 toDF 함수를 활용할 수 있어 스파크의 implicits가 주는 장점을 얻을 수 있음<br/>\n",
    "물론 implicits를 import해야 함<br/>\n",
    "하지만 implicits는 null 타입과 잘 맞지 않음<br/>\n",
    "그러므로 실제 운영 환경에서 사용하는 것은 권장하지 않음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myDF: org.apache.spark.sql.DataFrame = [col1: string, col2: int ... 1 more field]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myDF = Seq((\"Hello\", 2, 1L)).toDF(\"col1\", \"col2\", \"col3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 DataFrame을 만드는 방법을 알아보았음<br/>\n",
    "이제 다음과 같이 가장 유용하게 사용할 수 있는 메서드를 알아보겠음<br/>\n",
    "* 컬럼이나 표현식을 사용하는 select 메서드\n",
    "* 문자열 표현식을 사용하는 selectExpr 메서드\n",
    "* 메서드로 사용할 수 없는 org.apache.spark.sql.functions 패키지에 포함된 다양한 함수\n",
    "\n",
    "이 3가지 유형의 메서드로 DataFrame을 다룰 때 필요한 대부분의 transformation 작업을 해결할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 select와 selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, col, column}\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{expr, col, column}\n",
    "\n",
    "df.select(df.col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(column(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 객체와 문자열을 함께 섞어 쓰는 실수를 많이 함<br/>\n",
    "다음 코드 예제를 실행하면 컴파일러 오류가 발생함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "37: error: overloaded method value select with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:37: error: overloaded method value select with alternatives:",
      "  [U1, U2](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1], c2: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U2])org.apache.spark.sql.Dataset[(U1, U2)] <and>",
      "  (col: String,cols: String*)org.apache.spark.sql.DataFrame <and>",
      "  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame",
      " cannot be applied to (org.apache.spark.sql.Column, String)",
      "       df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")",
      "          ^",
      ""
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expr 함수는 가장 유연한 참조 방법임<br/>\n",
    "expr 함수는 단순 컬럼 참조나 문자열을 이용해 컬럼을 참조할 수 있음<br/>\n",
    "설명을 위해 AS 키워드로 컬럼명을 변경한 다음 alias 메서드로 원래 컬럼명으로 되돌려보겠음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 변경함 컬럼명을 원래 이름으로 되돌려 놓음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 select 메서드에 expr 함수를 사용하는 패턴을 자주 활용함<br/>\n",
    "스파크는 이런 작업을 간단하고 효율적으로 할 수 있는 *selectExpr* 메서드를 제공함<br/>\n",
    "*selectExpr* 메서드는 자주 사용하는 편리한 인터페이스 중 하나임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME AS newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selectExpr 메서드는 스파크의 진정한 능력을 보여줌<br/>\n",
    "selectExpr 메서드는 새로운 DataFrame을 생성하는 복잡한 표현식을 간단하게 만드는 도구임<br/>\n",
    "사실 모든 유효한 비집계형(non-aggregating) SQL 구문을 지정할 수 있음<br/>\n",
    "단, 컬럼을 식별할 수 있어야 함<br/>\n",
    "다음 코드는 DataFrame에 출발지와 도착지가 같은지 나타내는 새로운 withinCountry 컬럼을 추가하는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------------+\n",
      "|   DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME| count|withinCountry|\n",
      "+--------------------+--------------------+------+-------------+\n",
      "|       United States|             Romania|    15|        false|\n",
      "|       United States|             Croatia|     1|        false|\n",
      "|       United States|             Ireland|   344|        false|\n",
      "|               Egypt|       United States|    15|        false|\n",
      "|       United States|               India|    62|        false|\n",
      "|       United States|           Singapore|     1|        false|\n",
      "|       United States|             Grenada|    62|        false|\n",
      "|          Costa Rica|       United States|   588|        false|\n",
      "|             Senegal|       United States|    40|        false|\n",
      "|             Moldova|       United States|     1|        false|\n",
      "|       United States|        Sint Maarten|   325|        false|\n",
      "|       United States|    Marshall Islands|    39|        false|\n",
      "|              Guyana|       United States|    64|        false|\n",
      "|               Malta|       United States|     1|        false|\n",
      "|            Anguilla|       United States|    41|        false|\n",
      "|             Bolivia|       United States|    30|        false|\n",
      "|       United States|            Paraguay|     6|        false|\n",
      "|             Algeria|       United States|     4|        false|\n",
      "|Turks and Caicos ...|       United States|   230|        false|\n",
      "|       United States|           Gibraltar|     1|        false|\n",
      "|Saint Vincent and...|       United States|     1|        false|\n",
      "|               Italy|       United States|   382|        false|\n",
      "|       United States|Federated States ...|    69|        false|\n",
      "|       United States|              Russia|   161|        false|\n",
      "|            Pakistan|       United States|    12|        false|\n",
      "|       United States|         Netherlands|   660|        false|\n",
      "|             Iceland|       United States|   181|        false|\n",
      "|    Marshall Islands|       United States|    42|        false|\n",
      "|          Luxembourg|       United States|   155|        false|\n",
      "|            Honduras|       United States|   362|        false|\n",
      "|         The Bahamas|       United States|   955|        false|\n",
      "|       United States|             Senegal|    42|        false|\n",
      "|         El Salvador|       United States|   561|        false|\n",
      "|               Samoa|       United States|    25|        false|\n",
      "|       United States|              Angola|    13|        false|\n",
      "|         Switzerland|       United States|   294|        false|\n",
      "|       United States|            Anguilla|    38|        false|\n",
      "|        Sint Maarten|       United States|   325|        false|\n",
      "|           Hong Kong|       United States|   332|        false|\n",
      "| Trinidad and Tobago|       United States|   211|        false|\n",
      "|              Latvia|       United States|    19|        false|\n",
      "|       United States|             Ecuador|   300|        false|\n",
      "|            Suriname|       United States|     1|        false|\n",
      "|              Mexico|       United States|  7140|        false|\n",
      "|       United States|              Cyprus|     1|        false|\n",
      "|             Ecuador|       United States|   268|        false|\n",
      "|       United States|            Portugal|   134|        false|\n",
      "|       United States|          Costa Rica|   608|        false|\n",
      "|       United States|           Guatemala|   318|        false|\n",
      "|       United States|            Suriname|    34|        false|\n",
      "|            Colombia|       United States|   873|        false|\n",
      "|       United States|          Cape Verde|    14|        false|\n",
      "|       United States|             Jamaica|   712|        false|\n",
      "|              Norway|       United States|   121|        false|\n",
      "|       United States|            Malaysia|     3|        false|\n",
      "|       United States|             Morocco|    19|        false|\n",
      "|            Thailand|       United States|     3|        false|\n",
      "|       United States|               Samoa|    25|        false|\n",
      "|           Venezuela|       United States|   290|        false|\n",
      "|       United States|               Palau|    31|        false|\n",
      "|       United States|           Venezuela|   246|        false|\n",
      "|              Panama|       United States|   510|        false|\n",
      "| Antigua and Barbuda|       United States|   126|        false|\n",
      "|       United States|               Chile|   185|        false|\n",
      "|             Morocco|       United States|    15|        false|\n",
      "|       United States|             Finland|    28|        false|\n",
      "|          Azerbaijan|       United States|    21|        false|\n",
      "|       United States|              Greece|    23|        false|\n",
      "|       United States|         The Bahamas|   986|        false|\n",
      "|         New Zealand|       United States|   111|        false|\n",
      "|             Liberia|       United States|     2|        false|\n",
      "|       United States|           Hong Kong|   414|        false|\n",
      "|             Hungary|       United States|     2|        false|\n",
      "|       United States|               China|   920|        false|\n",
      "|       United States|             Vietnam|     2|        false|\n",
      "|        Burkina Faso|       United States|     1|        false|\n",
      "|              Sweden|       United States|   118|        false|\n",
      "|       United States|              Kuwait|    28|        false|\n",
      "|       United States|  Dominican Republic|  1420|        false|\n",
      "|       United States|               Egypt|    12|        false|\n",
      "|              Israel|       United States|   134|        false|\n",
      "|       United States|       United States|370002|         true|\n",
      "|            Ethiopia|       United States|    13|        false|\n",
      "|       United States|          Luxembourg|   134|        false|\n",
      "|       United States|              Poland|    33|        false|\n",
      "+--------------------+--------------------+------+-------------+\n",
      "only showing top 85 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"*\", // 모든 원본 컬럼 포함\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\n",
    "    .show(85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select 표현식에는 DataFrame의 컬럼에 대한 집계 함수를 지정할 수 있음<br/>\n",
    "다음 예제는 지금까지의 예제와 크게 다르지 않음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show() // show(1)이든 show(100)이든 결과는 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.3 스파크 데이터 타입으로 변환하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "때로는 새로운 컬럼이 아닌 명시적인 값을 스파크에 전달해야 함<br/>\n",
    "명시적인 값은 상수값일 수 있고, 추후 비교에 사용할 무언가가 될 수도 있음<br/>\n",
    "이때 **리터럴(literal)**을 사용함<br/>\n",
    "리터럴은 프로그래밍 언어의 리터럴 값을 스파크가 이해할 수 있는 값으로 변환함<br/>\n",
    "리터럴은 표현식이며 이전 예제와 같은 방식으로 사용함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).as(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 상수나 프로그래밍으로 생성된 변숫값이 특정 컬럼의 값보다 큰지 확인할 때 리터럴을 사용함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame에 신규 컬럼을 추가하는 공식적인 방법은 DataFrame의 withColumn 메서드를 사용하는 것임<br/>\n",
    "숫자 1을 값으로 가지는 컬럼을 추가하는 예제는 다음과 같음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 출발지와 도착지가 같은지 여부를 Boolean 타입으로 표현하는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2) // expr 안에 =을 써도 ==을 써도 같은 결과가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn 메서드는 2개의 인수를 사용함<br/>\n",
    "하나는 컬럼명이고, 다른 하나는 값을 생성할 표현식임<br/>\n",
    "한 가지 재미있는 것은 withColumn 메서드로 컬럼명을 변경할 수도 있다는 것임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination)\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.5 컬럼명 변경하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn 메서드 대신 withColumnRenamed 메서드로 컬럼명을 변경할 수도 있음<br/>\n",
    "withColumnRenamed 메서드는 첫 번째 인수로 전달된 컬럼명을 두 번째 인수의 문자열로 변경함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: Array[String] = Array(dest, ORIGIN_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.6 예약 문자와 키워드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공백이나 하이픈(-) 같은 예약 문자는 컬럼명에 사용할 수 없음<br/>\n",
    "예약 문자를 컬럼명에 사용하려면 백틱(`) 문자를 이용해 escaping해야 함<br/>\n",
    "withColumn 메서드를 사용해 예약 문자가 포함된 컬럼을 생성해보겠음<br/>\n",
    "다음은 escaping 문자가 필요한 경우와 필요 없는 경우의 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n",
       "dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "val dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서는 withColumn 메서드의 첫 번째 인수로 새로운 컬럼명을 나타내는 문자열을 지정했기 때문에 escape 문자가 필요 없음<br/>\n",
    "하지만 다음 예제에서는 표현식으로 컬럼을 참조하므로 백틱(`) 문자를 사용함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\")\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표현식 대신 문자열을 사용해 명시적으로 컬럼을 참조하면 리터럴로 해석되기 때문에 예약 문자가 포함된 컬럼을 참조할 수 있음<br/>\n",
    "예약 문자나 키워드를 사용하는 표현식에만 escape 처리가 필요함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res65: Array[String] = Array(This Long Column-Name)\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.select(col(\"This Long Column-Name\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.7 대소문자 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.8 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 스파크는 대소문자를 가리지 않음<br/>\n",
    "다음과 같은 설정을 사용해 스파크에서 대소문자를 구분하게 만들 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame에서 컬럼을 제거하는 방법을 알아보겠음<br/>\n",
    "select 메서드로 컬럼을 제거할 수 있지만 컬럼을 제거하는 메서드인 drop을 사용할 수도 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Array[String] = Array(DEST_COUNTRY_NAME, count)\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다수의 컬럼명을 drop 메서드의 인수로 사용해 컬럼을 한꺼번에 제거할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res68: Array[String] = Array(count, This Long Column-Name)\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.9 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가끔 특정 데이터 타입을 다른 데이터 타입으로 형변환할 필요가 있음<br/>\n",
    "다수의 StringType 컬럼을 정수형으로 변환해야 하는 경우가 그 예임<br/>\n",
    "cast 메서드로 데이터 타입을 변환할 수 있음<br/>\n",
    "다음은 count 컬럼을 Integer 데이터 타입에서 String 데이터 타입으로 형변환하는 예제임<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.10 로우 필터링하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로우를 필터링하려면 참과 거짓을 판별하는 표현식을 만들어야 함<br/>\n",
    "그러면 표현식의 결과가 false인 로우를 걸러낼 수 있음<br/>\n",
    "*DataFrame의 가장 일반적인 필터링 방법은 문자열 표현식이나 컬럼을 다루는 기능을 이용해 표현식을 만드는 것임<br/>\n",
    "DataFrame의 where 메서드나 filter 메서드로 필터링할 수 있음*<br/>\n",
    "이 두 메서드 모두 같은 연산을 수행하며 같은 파라미터 타입을 사용함<br/>\n",
    "이 중 SQL과 유사한 where 메서드를 앞으로 계속 사용하겠지만, filter도 사용할 수 있다는 점을 기억하기 바람<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*<br/>\n",
    "스칼라나 자바에서 Dataset API를 이용해 filter 메서드를 사용하면 Dataset의 각 레코드에 적용할 함수를 filter 메서드에 사용할 수 있음<br/>\n",
    "자세한 내용을 11장을 참조할 것<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예제의 filter와 where 메소드는 모두 동일하게 동작함<br/>\n",
    "스칼라와 파이썬 모두 같은 결과를 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 표현식에 여러 필터를 적용해야 할 때도 있음<br/>\n",
    "하지만 스파크는 자동으로 필터의 순서와 상관없이 동시에 모든 필터링 작업을 수행하기 때문에 항상 유용한 것은 아님<br/>\n",
    "그러므로 여러 개의 AND 필터를 지정하려면 차례대로 필터를 연결하고 판단은 스파크에 맡겨야 함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") =!= \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.11 고유한 로우 얻기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 DataFrame에서 고윳값이나 중복되지 않은 값을 얻는 연산을 자주 사용함<br/>\n",
    "DataFrame의 모든 로우에서 중복 데이터를 제거할 수 있는 distinct 메서드를 사용해 고윳값을 찾을 수 있음<br/>\n",
    "항공운항 데이터셋에서 중복되지 않은 출발지 정보를 얻는 예제를 살펴보겠음<br/>\n",
    "distinct 메서드는 중복되지 않은 로우를 가진 신규 DataFrame을 반환함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res73: Long = 256\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: Long = 125\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.12 무작위 샘플 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame에서 무작위 샘플 데이터를 얻으려면 DataFrame의 sample 메서드를 사용함<br/>\n",
    "DataFrame에서 표본 데이터 추출 비율을 지정할 수 있으며, 복원 추출이나 비복원 추출의 사용 여부를 지정할 수도 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 5\n",
       "withReplacement: Boolean = false\n",
       "fraction: Double = 0.5\n",
       "res78: Long = 138\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 5\n",
    "val withReplacement = false\n",
    "val fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.13 임의 분할하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의 분할(random split)은 원본 DataFrame을 임의 크기로 '분할'할 때 유용하게 사용됨<br/>\n",
    "이 기능은 머신러닝 알고리즘에서 사용할 학습셋, 검증셋, 그리고 테스트셋을 만들 때 주로 사용함<br/>\n",
    "다음 예제에서는 분할 가중치를 함수의 파라미터로 설정해 원본 DataFrame을 서로 다른 데이터를 가진 두 개의 DataFrame으로 나눔<br/>\n",
    "이 메서드는 임의성을 가지도록 설계되었으므로 시드값을 반드시 설정해야 함<br/>\n",
    "총합이 1이 되도록 각 DataFrame의 비율을 지정하지 않으면 예제와 같은 비율로 지정됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFrames: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field], [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field])\n",
       "res79: Boolean = false\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataFrames = df.randomSplit(Array(0.25, 0.75), seed)\n",
    "dataFrames(0).count() > dataFrames(1).count() // False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*보충 설명*\n",
    "\n",
    "번역본이어서 매끄럽지 않은 부분도 있고 이 부분에 대한 책 설명이 다소 부족하다고 느껴져서 [여기](https://sparkbyexamples.com/spark/spark-sampling-with-examples/)를 참고한 내용을 덧붙여보겠음<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "anotherDF: org.apache.spark.sql.Dataset[Long] = [id: bigint]\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val anotherDF = spark.range(100)\n",
    "println(anotherDF.collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 fraction은 dataFrame에서 대략 어느 정도 비율만큼 샘플링을 할 것이냐를 의미하는 파라미터임<br/>\n",
    "하지만 fraction을 지정한다고 딱 그만큼의 비율을 샘플링하는 것은 아니고 대략 그 정도만큼을 샘플링하게 됨<br/>\n",
    "아래 예시를 보면 같은 dataFrame에 대해 같은 fraction 값을 썼지만 출력되는 값의 개수는 다른 것을 볼 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,33,56,66,69,76\n"
     ]
    }
   ],
   "source": [
    "println(anotherDF.sample(0.1).collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,14,33,40,43,63,65,79,87,92\n"
     ]
    }
   ],
   "source": [
    "println(anotherDF.sample(0.1).collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed의 경우 sample 메서드로 샘플링된 결과를 여러 번 부를 때 sample 메서드 자체의 임의성에도 불구하고 항상 같은 결과를 얻기 위해 사용하는 것임<br/>\n",
    "즉 seed 값을 같게 하면 sample 메서드로 샘플링된 결과가 같게 함으로써 임의성을 가진 sample 메서드로부터 항상 같은 결과를 얻는 것임<br/>\n",
    "물론 seed 값을 다르게 하면 다른 결과를 얻게 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,26,29,33,35,37,38,42,46,48,50,61,73,85,89\n"
     ]
    }
   ],
   "source": [
    "println(anotherDF.sample(0.1, 2).collect().mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,26,29,33,35,37,38,42,46,48,50,61,73,85,89\n"
     ]
    }
   ],
   "source": [
    "println(anotherDF.sample(0.1, 2).collect().mkString(\",\")) // 같은 시드값을 쓰기 때문에 위와 같은 샘플링 결과를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,19,21,30,38,41,56,69,84\n"
     ]
    }
   ],
   "source": [
    "println(anotherDF.sample(0.1, 37).collect().mkString(\",\")) // 다른 시드값을 쓰기 때문에 위와 다른 샘플링 결과를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.14 로우 합치기와 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배웠듯이 DataFrame은 불변성을 가짐<br/>\n",
    "그러므로 DataFrame에 레코드를 추가하는 작업은 DataFrame을 변경하는 작업이기 때문에 불가능함<br/>\n",
    "DataFrame에 레코드를 추가하려면 원본 DataFrame을 새로운 DataFrame과 **통합(union)**해야 함<br/>\n",
    "통합은 두 개의 DataFrame을 단순히 결합하는 행위임<br/>\n",
    "통합하려는 두 개의 DataFrame은 반드시 동일한 스키마와 컬럼 수를 가져야 함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CAUTION*<br/>\n",
    "union 메서드는 현재 스키마가 아닌 컬럼 위치를 기반으로 동작함<br/>\n",
    "따라서 사용자가 생각한 대로 자동 정렬되지 않을 수도 있음<br/>\n",
    "-> 이 부분은 아직 무슨 뜻인지 잘 이해가 안 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n",
       "newRows: Seq[org.apache.spark.sql.Row] = List([New Country,Other Country,5], [New Country 2,Other Country 3,1])\n",
       "parallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[328] at parallelize at <console>:58\n",
       "newDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val schema = df.schema\n",
    "\n",
    "val newRows = Seq(\n",
    "    Row(\"New Country\", \"Other Country\", 5L), // 이 신규 로우는 5L이므로 count값이 5\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1L) // 이 신규 로우는 1L이므로 count값이 1\n",
    ")\n",
    "\n",
    "val parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "val newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "df.union(newDF)\n",
    "    .where(\"count = 1\")\n",
    "    .where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\")\n",
    "    .show() // 전체 데이터를 조회하면 신규 로우를 확인할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로우가 추가된 DataFrame을 참조하려면 새롭게 만들어진 DataFrame 객체(예제 기준으로는 newDF)를 사용해야 함<br/>\n",
    "DataFrame을 뷰로 만들거나 테이블로 등록하면 DataFrame 변경 작업과 관계없이 동적으로 참조할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.15 로우 정렬하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort와 orderBy 메서드를 사용해 DataFrame의 최댓값 혹은 최솟값이 상단에 위치하도록 정렬할 수 있음<br/>\n",
    "두 메서드는 완전히 같은 방식으로 동작함 (스파크 코드를 살펴보면 orderBy 메서드 내부에서 sort 메서드를 호출함)<br/>\n",
    "두 메서드 모두 컬럼 표현식과 문자열을 사용할 수 있으며 다수의 컬럼을 지정할 수 있음<br/>\n",
    "*기본 동작 방식은 오름차순 정렬임*<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정렬 기준을 명확히 지정하려면 asc나 desc 함수를 사용함<br/>\n",
    "두 함수 모두 컬럼의 정렬 순서를 지정함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드를 사용하여 정렬된 DataFrame에서 null 값이 표시되는 기준을 지정할 수 있음 <br/>\n",
    "\n",
    "transformation을 처리하기 전에 성능을 최적화하기 위해 파티션별 정렬을 수행하기도 함<br/>\n",
    "파티션별 정렬은 sortWithinPartitions 메서드로 할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res103: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"Downloads/Spark-The-Definitive-Guide/data/flight-data/json/*-summary.json\")\n",
    "    .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "튜닝과 최적화 내용은 3부(저수준 API; 12~14장)에서 자세히 알아보겠음<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.16 로우 수 제한하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame에서 추출할 로우 수를 제한해야 할 때가 있음<br/>\n",
    "DataFrame에서 상위 10개의 결과만을 보고자 하는 경우가 그 예임<br/>\n",
    "limit 메서드를 사용해 추출할 로우 수를 제한할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "|   United Kingdom|      United States|  2025|\n",
      "+-----------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(\"count\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.17 repartition과 coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 최적화 기법은 자주 필터링하는 컬럼을 기준으로 데이터를 분할하는 것임<br/>\n",
    "이를 통해 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적인 데이터 구성을 제어할 수 있음<br/>\n",
    "\n",
    "repartition 메서드를 호출하면 무조건 전체 데이터를 셔플함<br/>\n",
    "향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에만 사용해야 함 -> 왜? 잘 이해가 안 됨<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res106: Int = 1\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions // 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res107: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 컬럼을 기준으로 자주 필터링한다면 자주 필터링되는 컬럼을 기준으로 파티션을 재분배하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res108: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선택적으로 파티션 수를 지정할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res109: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coalesce 메서드는 전체 데이터를 셔플하지 않고 파티션을 병합하려는 경우에 사용함<br/>\n",
    "(파티션 수를 즐이려면 셔플이 일어나는 repartition 대신 coalesce를 사용해야 함)<br/>\n",
    "다음은 목적지를 기준으로 셔플을 수행해 5개의 파티션으로 나누고, 전체 데이터를 셔플 없이 병합하는 예제임<br/>\n",
    "\n",
    "*의문사항<br/>\n",
    "그냥 셔플을 수행하는 것과 목적지(특정 컬럼)를 기준으로 셔플을 수행하는 것의 차이점?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res110: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.18 드라이버로 로우 데이터 수집하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크는 드라이버에서 클러스터 상태 정보를 유지함<br/>\n",
    "로컬 환경에서 데이터를 다루려면 드라이버로 데이터를 수집해야 함<br/>\n",
    "\n",
    "아직 드라이버로 데이터를 수집하는 연산을 정확하게 설명하지 않았음<br/>\n",
    "하지만 몇 가지 메서드는 이미 사용해보았음<br/>\n",
    "collect 메서드는 전체 DataFrame의 모든 데이터를 수집하며, take 메서드는 상위 N개의 로우를 반환함<br/>\n",
    "show 메서드는 여러 로우를 보기 좋게 출력함<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collectDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "res112: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344], [Egypt,United States,15], [United States,India,62])\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val collectDF = df.limit(10)\n",
    "collectDF.take(5) // take는 정수형 값을 인수로 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res115: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344], [Egypt,United States,15], [United States,India,62], [United States,Singapore,1], [United States,Grenada,62], [Costa Rica,United States,588], [Senegal,United States,40], [Moldova,United States,1])\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터셋에 대한 반복(iterate) 처리를 위해 드라이버로 로우를 모으는 또 다른 방법이 있음<br/>\n",
    "toLocalIterator 메서드는 이터레이터(iterator)로 모든 파티션의 데이터를 드라이버에 전달함<br/>\n",
    "toLocalIterator 메서드를 사용해 데이터셋의 파티션을 차례로 반복 처리할 수 있음<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res116: java.util.Iterator[org.apache.spark.sql.Row] = IteratorWrapper(<iterator>)\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CAUTION*<br/>\n",
    "드라이버로 모든 데이터 컬렉션을 수집하는 작업은 매우 큰 비용(CPU, 메모리, 네트워크 등)이 발생함<br/>\n",
    "대규모 데이터셋에 collect 명령을 수행하면 드라이버가 비정상적으로 종료될 수 있음<br/>\n",
    "toLocalIterator 메서드도 마찬가지임<br/>\n",
    "toLocalIterator 메서드를 사용할 때 매우 큰 파티션이 있다면 드라이버와 애플리케이션이 비정상적으로 종료될 수 있음<br/>\n",
    "또한 연산을 병렬로 수행하지 않고 차례로 처리하기 때문에 매우 큰 처리 비용이 발생함<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서는 DataFrame의 기본 연산을 알아보았음<br/>\n",
    "그리고 DataFrame을 사용하는 데 필요한 개념과 다양한 기능도 함께 알아보았음<br/>\n",
    "다음 장에서는 DataFrame의 데이터를 다루는 다양한 방법을 자세히 알아보겠음<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
